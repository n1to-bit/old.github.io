<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on nito95 blog</title>
    <link>https://nito95.github.io/posts/</link>
    <description>Recent content in Posts on nito95 blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sun, 11 Jul 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://nito95.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>AWS Auroraは何がすごいのか</title>
      <link>https://nito95.github.io/posts/2021-07-11/</link>
      <pubDate>Sun, 11 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://nito95.github.io/posts/2021-07-11/</guid>
      <description>最近お手伝いしているベンチャーでRDS PostgreSQLをAuroraに移行しました。本番環境がスタンバイレプリカがない状態でRDSインスタンス1台で動いており、可用性を高めた方がいいのでどうせならAuroraに移行しちゃいましょう、と提案しエイヤと移行しました。
その話を友人にしたら、「Auroraって何がすごいん?」と質問されました。前職でも当然のようにAuroraを使っており、Auroraは可用性が高くwriteヘビーなサービスにも強くて少しコストが高い、というザックリしたイメージしかなかったので、フワっとした返答しかできませんでした。
ちゃんと調べてみようと思い、論文 Amazon Aurora: Design Considerations for High Throughput Cloud-Native Relational Databases を読んで後日改めて友人に説明したので、そのノリでここにもまとめてみます。
耐障害性とQuorum ストレージは1つで稼働させていると、何かの障害でそのストレージが止まってしまうとサービス全体が止まってしまうことになります。それを回避するために同じシステム環境を2つ用意し、1つは稼働し1つは待機させ、障害が発生した際には待機させていたストレージに稼働を切り替えるだけでサービスが再開するようにして可用性を高めます。いわゆるレプリケーションです。
この2つは常に同じデータを保持していなければなりません。なのでデータを書き込む際には両方に対して書き込みをするわけですが、片方のストレージで障害が起きたりネットワークの不通で書き込みに失敗した場合には2つのデータの整合性を保ちたいのでリトライが行われます。
しかしパフォーマンス面からするとリトライ処理は悪なので、1つくらいストレージが不調でレスポンスが返ってこなくても気にせず進めたい。データの整合性は保ちつつパフォーマンスも下げたくないとなると、解決策としては3つストレージを配置することとなります。 書き込みで3つ並列にリクエストが行い、1つ不調だったとしても2つからレスポンスが返ってこれば不整合なく書き込みが成功したとし、読み込みは3つにリクエストを行い2つから同じレスポンスが返ればそれは最新のデータだとします。多数決が行われるわけです。1つ遅いストレージ(straggler)がいても残り2つが正常なら素早い合意が取れるのでパフォーマンスの面でもメリットがあります。
分散システムにおいて処理の整合性をとるために多数決でもって合意を行うことをQuorum（クオラム）モデルと言います。
しかしここで終わりません。3つに分散させて2/3 Quorumで合意する仕組みでは不十分だとしました。
Auroraは6台構成 AWSの各リージョンには必ず3つ以上のAZ(アベイラビリティーゾーン)があります。AZ同士は物理的に距離が離れており、使われている電源系統も違えばソフトウェアのデプロイ周期も違います。 AWSの障害はAZ単位で起こることが多いので、サーバーを2つ作る場合は1つはAZ-aに置いて、もう1つはAZ-bに置けばAZ-aで障害が起きてもサービスは止まらずに済みます。
先ほど書いた2/3 Quorumでデータベースを構築するならば、AZ-aとAZ-bとAZ-cにそれぞれストレージを1つずつ置きます。ここでもしAZ-aで大規模な火災が起きたらどうなるでしょうか。復旧は数日ではできないので、しばらくAZ-bとAZ-cだけで稼働することになります。ここでさらにAZ-bに置いたストレージ1つに一時的な障害が起きてしまうと、稼働できるのがAZ-cだけとなり、読み込みも書き込みも2/3を満たせず合意が取れなくなります。
このAZ+1障害(AZレベルの障害が起き、さらにストレージ1つに障害が起きる)に対応するためにAuroraでは6台構成とし、4/6の書き込みQuorumと3/6の読み込みQuorumとしました。 各AZに2台ずつ置くことでAZ障害が起きても4つのストレージで読み書きでき、さらに1つ不調となっても3つのストレージで読み込みだけはできるようになります。
なぜ書き込みが4/6で読み込みが3/6なのか　 全体の投票数(ストレージ数)を \(V\)とし、読み込みQuorumの投票数を\(V_{r}\) 書き込みQuorumの投票数を\(V_{w}\)とします。
書き込みでは最新のデータを競合を回避して書き込めることが求められるので、過半数である\(V_{w} &amp;gt; V/2\)を満たす必要があります。もし半分だったらどっちが正解かわからなくなります。一方、読み込みは最新の書き込まれたデータを読むことが求められるので \(V_{r} + V_{w} &amp;gt; V\)を満たす必要があります。書き込みでは過半数を超えたら成功するので、このルールにすることで読み込みQuorumの中には少なくとも1つは最新のデータが必ず含まれることになります。
元々 hoge というデータが6つのストレージそれぞれに入っている状態で、 fuga に書き換えるとします。6つのうち4つは書き込みが成功した一方で2つは書き込みが失敗しました。結果的には過半数を超えるので書き込み成功です。すると次に読み込みを行うと4つからは fuga 2つからは hoge が返ってきます。最新データの取得を保証する意味においては3つから fuga が返ってこれば問題ないわけです。3つから hoge 3つから fuga が返ってくることは書き込みが成功している時点であり得ません。
  分散させることで生じるNetwork I/Oの問題と解決策 ストレージを複数に分散させる場合、問題になるのはNetworkです。従来のレプリケーションされたMySQLでは、書き込みのためにredo log、binlog、doublewrite用のデータページ、メタデータ(FRM)を送る必要があるため、6台構成ではとんでもない量のトラフィックとなってしまいます。 そのためAuroraでは、書き込みの際にネットワークを超えて送られるデータはredo logのみとし、ストレージ層にlog applicatorを置き各ストレージが自立的にredo logを再生することで最新状態までデータリカバリーできるようにしています。
Write-Ahead Log(WAL)とRedo Logとは WALは日本語にするとログ先行書き込みで、データベースに対する処理を行う前に全てログに記録する手法をいいます。例えばUPDATEをかけた場合、データをディスクから読み出してメモリ上で値を変えてディスクにフラッシュする必要があります。これが大量のレコードだった場合はデータをある程度の大きさのチャンクに分けてその塊ごとにUPDATEをかけていきます。 もしその途中で電源が落ちた場合、一部だけ書き換わっていて残りは古いままとなる不整合が発生します。それを防ぐために、書き込みをする前にどのページをどう書き換えるか操作情報を全てログに書きます。電源が落ちて再び電源を入れる際にクラッシュリカバリが走り、ログをチェックし、本来行うべき内容と実際に行われた内容を比較して処理を開始時までundoするか、redoして再開させるか判断することができます。ファイルシステムでいうジャーナリングと似てます。</description>
    </item>
    
    <item>
      <title>C&#43;&#43;の std::sort() で使われている Introsort とは</title>
      <link>https://nito95.github.io/posts/2021-07-02/</link>
      <pubDate>Fri, 02 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://nito95.github.io/posts/2021-07-02/</guid>
      <description>書籍 「問題解決力を鍛える!アルゴリズムとデータ構造」 を読み切ったのですが、C++ STLの sort() には Introsort というアルゴリズムが使われていると載っているだけで、具体的にどのように実装されているのか詳細は書かれていませんでした。
std::sort() の Syntaxは
sort(first, last, comp) first, last: Random-access iterators to the initial and final positions of the sequence to be sorted. comp: Binary function that accepts two elements in the range as arguments, and returns a value convertible to bool. Time Complexity は
 Best Case – O(N log N) Average Case - O(N log N) Worse Case - O(N log N)  で、安定ソート(同じ値の順序が維持されるソート)ではないです。</description>
    </item>
    
    <item>
      <title>「コンピュータシステムの理論と実装」(Nand2Tetris)</title>
      <link>https://nito95.github.io/posts/2021-04-12/</link>
      <pubDate>Mon, 12 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://nito95.github.io/posts/2021-04-12/</guid>
      <description>低レイヤーの知識を幅広く身に着けようと思い、1ヶ月前から 「コンピュータシステムの理論と実装 ― モダンなコンピュータの作り方」 を読んで実践していました。
最初にNandゲートを学んでから積み上げ式でALU、CPU、メモリ、アセンブラ、VM変換器、コンパイラ、OSを一気通貫で作っていくという本です。本自体はあまり大きくなくサックリできるかなと思って始めたのですが、非常にHeavyで濃密な時間を過ごすこととなりました。
この本は今後もたまに読み返すことになると思います。
  ちなみにこの本を読む前の自分の低レイヤーに関する知識は、
 「プログラムはなぜ動くのか」 「コンピュータはなぜ動くのか」 「［試して理解］Linuxのしくみ」 「コーディングを支える技術」  を読んでいるレベルです。あと遊びでアセンブリでHello Worldを出力させたことがある程度でした。(参考: Macでアセンブリ言語のhello worldを実行する方法)
大学で情報系の勉強をしていない自分は、論理ゲートやフリップフロップですら新鮮でした。この本ではハードウェアシミュレータを使って自分で作ったものを実際に動かすので非常に腑に落としやすく、定着度も深いものとなります。
この本は各トピックの概要レベルしか理解できず、作るものもモダンなものとは程遠いと思いますが、ここからスタートして次に各トピックに特化された本を読むといいと思います。 CPUは「CPUの創りかた」 コンパイラは 「低レイヤを知りたい人のためのCコンパイラ作成入門」 OSは 「Operating-Systems-Three-Easy-Pieces」あたりで深める予定です。
あと第6章のアセンブラと第7~8章のVM変換器は、競プロ用に使っているまだ経験の浅いC++で書いてみたので、C++のファイル操作や文字列操作の知見を深めるいいきっかけになりました。コードは github にあげています。</description>
    </item>
    
    <item>
      <title>AtCoder C&#43;&#43;入門 APG4b</title>
      <link>https://nito95.github.io/posts/2021-03-02/</link>
      <pubDate>Tue, 02 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://nito95.github.io/posts/2021-03-02/</guid>
      <description>前職の頃、仕事中の気分転換に同僚とLeetCodeをたまーにやっていました。前職のメイン言語がGoとRubyだったのでRubyで解いていました。アルゴリズムは全く勉強しておらず、Easyは一通り解けてMediumは難しいなあ、というレベルでした。
あれから1~2年経ち、ちゃんと勉強して競プロに参加してみようと思い、どうせ始めるならC++を勉強してC++で書いていこう！ということで AtCoderの APG4b をやりました。
APG4bとは APG4b はAtCoderのC++入門教材です。C++の基本文法や、再帰、ビット全探索など競プロで使う手法をわかりやすく説明し、その後練習問題を解いて定着を図るというものです。
環境構築 VSCodeでコーディング、コンパイル、実行できるようにしました。Code Runner というExtensionでVSCodeからコマンドを叩くことでコンパイル&amp;amp;実行できます。
少し詰まったこととして、AtCoderでは通るがローカルではコンパイルで失敗することがあり、調べた結果Code Runnerの実行コードを少し変えて
&amp;#34;cpp&amp;#34;: &amp;#34;cd $dir &amp;amp;&amp;amp; g++ $fileName -o $fileNameWithoutExt &amp;amp;&amp;amp; $dir$fileNameWithoutExt&amp;#34;, を
&amp;#34;cpp&amp;#34;: &amp;#34;cd $dir &amp;amp;&amp;amp; g++ -std=c++17 $fileName -o $fileNameWithoutExt &amp;amp;&amp;amp; $dir$fileNameWithoutExt&amp;#34;, にすることで通るようになりました。記憶が曖昧ですがvector周りの何かで失敗してたかな&amp;hellip;? 後のバージョンでサポートされた何かを含んでいたってことですね。
感想 全てを終えるのに1週間ほどかかりました。
C++の基本文法のインプットに手こずることはなく、練習問題も基本的にすんなり解けたのですが、よかったらこれも解いてみてねの感じで登場するABC/ARCの問題を解くのに非常に時間がかかり、結果1週間かかりました。結局全ての問題を解けたわけではないです。
特に再帰関数を使ったABC/ARCの問題は難しかったです。解いてる問題が圧倒的に少なく、まだ慣れていないというのが大きいと思うので、詰まったら悶々と考えるのではなく他の人の解答を見る、デバッグして変数の中身を見ながら処理を追ってみる、それでもよく分からなかったらいつか理解できるでしょと未来の自分に託す、という感じでやりました。
C++という言語について思ったこと 言語仕様が多いなーと思いました。APG4bでは最低限必要な仕様についてしかおそらく載っていないんだと思いますが、それでも複雑だなと思うことがチラホラありました。
C++のWikipedia には
 C++はCにクラスのサポートを追加しただけでなく、さらに次のような多種多様な機能を持っており、言語仕様は大変複雑である。言語処理系すなわちコンパイラの実装も、Cなどと比べて難易度が非常に高い。 多重継承 テンプレート 演算子のオーバーロード 例外処理 実行時型情報 (RTTI)  ここから、よりオブジェクト指向を強化し、「なんでもあり」ではない代わりにシンプルで分かりやすくスマートな設計を目指した新たな言語（JavaやD言語など）が作られることとなった。 とあり、なるほどなと思いました。Javaも後から機能を追加し続けて結果C++と似た道を歩んでないかな? と思いましたが :(
今後 書籍 「問題解決力を鍛える!アルゴリズムとデータ構造」 を買いました。初心者にも分かりやすい良本らしいです。まずはこれを読破します。
その後はAtCoderのABCの過去問を解いていこうかなと思います。解いたことのない問題が競プロに出ても全く歯が立たないと思うので、まずは数をこなして後に競プロに挑戦します。</description>
    </item>
    
    <item>
      <title>AWS CDK (TS) で prod/staging 環境を分けて Fargate 構築</title>
      <link>https://nito95.github.io/posts/2020-11-24/</link>
      <pubDate>Tue, 24 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://nito95.github.io/posts/2020-11-24/</guid>
      <description>AWS CDK (TS) を使って prod/staging でスタック名を分けてFargateの環境構築をしてみました。
デプロイするものは GET /health_check のみのエンドポイントを持つGo製のアプリです。
コードは こちら
AWS ECS Fargate とは ECSはコンテナ化されたアプリケーションを簡単にデプロイ、管理、スケーリングできるコンテナオーケストレーションサービスです。 Fargateはそのコンテナを実行する環境をサーバーレスにするサービスで、EC2インスタンスやそのスケーリングといった管理する必要がなくなります。
小さなアプリケーションをサクっと作ってデプロイするには Fargate は非常に便利です。
AWS CDK とは CDKはCloud Formation テンプレートを好きな言語で書くことができるDevelopmern Kitです。
Cloud Formation はAWSのシステム構成をJSONで記述したものです。コード化することで簡単に構築、修正、再利用できます。
アプリ実装 アプリ自体は本質ではないので何でもよかったのですが、個人的な趣味でGo製にし、フレームワークとして Echo を使ってます。
Fargateにデプロイするためにコンテナ化が必要なので、ローカル開発用の Dockerfile.local とデプロイ用の Dockerfile.prod の2つのDockerfileを用意します。 Dockerfile は [Qiita] Go 1.12 の開発環境と本番環境の Dockerfile を考える を参考にしています。
次にローカル開発用にdocker-compose.ymlを作り、servicesに Dockerfile.local を image に指定する app と postgres を記述します。
エンドポイントはヘルスチェックしかない単純なアプリですが、重要な部分は環境変数 ADDR と DATABASE_URL を起動時に取得し、取得できなければpanicで落ちる部分です (コード)。 後に作る Task Definition で環境変数の設定が必要となります。 DATABASE_URL は環境によってデータベースが異なるのでここの値も変わります。</description>
    </item>
    
    <item>
      <title>CでTCPソケットプログラミング</title>
      <link>https://nito95.github.io/posts/2020-04-10/</link>
      <pubDate>Fri, 10 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://nito95.github.io/posts/2020-04-10/</guid>
      <description>今関わっているプロジェクトで、3rd Party製のドラレコから流れるGPSと動画データを弊社のプラットフォームに投げ込むアダプターの開発をしています。
この3rd Partyデバイスとの通信方法が独自規格でして。。。通信方法について書かれたPDFがあり、こうやって通信を開始してね、このタイミングでACKしてね、危険運転があったらこういうバイナリを送るよ、とか詳細に書いてあるわけです。
ベテランのバックエンドエンジニアの人やファームウェアの人が通信部分をゴリゴリ作ってくださったのですが、コードを読んでも何がなんだかサッパリわかりませんでした。Ruby言語自体のコードの中身を見て、そこに書かれたCのコードを読んで、システムコールの仕様を調べては「よくわからんぞ」となっておりました。 そのときに「ソケットプログラミングしてみるといいよ」とアドバイスをもらったので、C言語でTCPソケットを実装してお勉強してみました。
ちなみにネットワークに関しての知識は
 「マスタリング TCP/IP 入門編」 「Real World HTTP - 歴史とコードに学ぶインターネットとウェブ技術」  の書籍を読んだことがある程度です。
TCPプログラミング こちらのサイトを参考にさせていただきました Geekなぺーじ Linuxネットワークプログラミング
サーバー サーバーはクライアントからの接続要求を待ちますが、どのように待つか設定します。
 ソケットを作る IPアドレスとポートを設定する クライアントから通信接続要求が来るまでプログラムを停止し接続後にプログラムを再開する データを送信する ソケットを閉じて通信接続を終了する  クライアント クライアントは特定のIPアドレス、TCPポート番号で接続待ちをするサーバーに対して接続要求を出します。
 ソケットを作る 接続先の指定 接続する サーバーからデータを受信  今回作ったもののソケットAPI callのサーバー、クライアントの関係を表すとこんな感じです。
  ソースコードはこちら nito95/clang-socket-programming
サーバーもクライアントもローカルなのでクライアント側で設定した接続先のIPは127.0.0.1ですが、ここは本来 gethostbyname でWinSockに名前解決を任せます。
これはサーバーとクライアントが繋がるとサーバーからクライアントにメッセージが送られ接続が切れる、という片手落ちなものです。 サーバーとしてあるべきはマルチスレッドで複数のクライアントと接続し、クライアントからの要求を受けたらクライアントにメッセージを返し、接続を切り再び接続要求待ち状態にするのがよいですね。
初めてOSI参照モデルでいうL3まで下りて理解が深まりました。これ以降は全然作り込んではないですが、もしCで基本的なライブラリだけでサーバーとクライアントを作ってみる系の良い教材があれば試してみたいなと思います。</description>
    </item>
    
    <item>
      <title>ブロックチェーンサービスでのMetaMaskログイン機能の実装方法 (Rails &#43; Vue.js)</title>
      <link>https://nito95.github.io/posts/2020-03-21/</link>
      <pubDate>Sat, 21 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://nito95.github.io/posts/2020-03-21/</guid>
      <description>最近NFTのマーケットプレイスサービスを友人のスタートアップで作っているのですが、お勉強がてら MetaMask ログイン機能を作ってみました。
パスワード不要で nonce 値を使った MetaMask 署名でのログインとなります。
完成イメージ MetaMask は ERC20準拠のトークンを保管するWebウォレットです。 ユーザー体験的にはGoogleやGithubなどのソーシャルログインと似ています。
  これはRails &amp;amp; Vue.jsで作っており、ソースコードはGithubにあげています → nito95/metamask-login-rails-vue
仕組み フロントがウォレットの秘密鍵を使って署名をすることでアカウントの所有権を証明します。バックエンドで署名済みデータ、ウォレットアドレス、nonce 使って検証できればそのウォレットアドレスはユーザー所有物だと認証でき、JWTを発行して以降の通信を行います。
詳細の実装を説明していきます。
まずはバックエンドで public_address と nonce を持った users テーブルを作成します。 public_address にはユーザーのウォレットアドレスを格納し、 nonce にはユーザー作成時&amp;amp;ログイン時に数字列をランダム生成した nonce 値を格納します。 public_address には UNIQUE制約をつけます。
認証フローの最初は、フロントで web3.eth.coinbase で MetaMask の現在アカウントのウォレットアドレスを取得します。 MetaMask とは web3.js を使ってやりとりができます。 アドレスを用いてバックエンドに対し GET /api/users?public_address=0xcA540... のようにリクエストを行い nonce を取得します。 当然このAPIは認証なしで成功するので public にしてよいデータしか返すべきではありません。 (demoのコードはview層を書かずにuserオブジェクトをそのまま返しているのでidが返っていますが、返すべきではないです)
users レコードがない場合はフロントは POST /api/users で public_address を送りユーザーアカウントを作成します。 バックエンドではランダム生成した nonce と共に users レコード作成し、フロントに nonce を返します。</description>
    </item>
    
    <item>
      <title>Design Docを書く文化</title>
      <link>https://nito95.github.io/posts/2020-01-14/</link>
      <pubDate>Tue, 14 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://nito95.github.io/posts/2020-01-14/</guid>
      <description>今所属しているベンチャーで、何か機能を開発する際にはまずDesign Docを書いていこうという文化ができてきました。
それまでは Jira に &amp;ldquo;〜機能&amp;rdquo; のような抽象的な大きなタスクを切って、そこからサブタスクを切って消化していくだけでしたが、まず最初にDesign Docを作るというタスクから始まるようになりました。
Design Docとは 一般的なDesign Docは設計仕様書のことですが、今参考にしているDesign DocはGoogleで使われているDesign Docです。弊社では
 目的 背景 スコープ 達成すべき要件と優先度付け (MUST, OPTIONALなどRFC的に) システム設計 UI設計 タスク出しと簡単な見積もり  などについて書きます。
そしてチームメンバーに共有し、これで行こうとなってから初めてタスクをスプリントに乗せることになります。
ちなみに参考にしたGoogleのDesign Docのテンプレートは こちら
意義 最初はちょっと面倒だなーと思っていましたが、今はメリットをひしひしと実感してます。
まずは &amp;ldquo;自分の脳内整理のための道具&amp;rdquo; の側面です。ユーザーからの要望がCSチームからチケットで上がり、CSの温度感が高いのでこういう機能を作ろう、となるケースが多かったのですが、Design Docを書いているときに これ本当に必要か? これが目的だから別の方法でも解決できるよね など、ユーザーに価値を届けるために本当にそれがベストなのか考えるきっかけとなります。 エンジニアをしているとコードをガリガリ書いていきたいのでつい新機能を作っていく方向になりがちですが、そもそも作らなくてよい方法があればそのほうがメンテコストはかからないし別のことに時間を割けます。
あと &amp;ldquo;他人への共有&amp;rdquo; の側面です。他のエンジニアがPRレビューするときに目的や背景がわかっていないと適切なレビューはできないので、PRには必ずDesign Docのリンクを貼ってレビュー時にも見てもらうようにしています。また、他人が意味するところに 将来の自分や将来メンテする他人 も含まれます。 機能が使われなくなったり、負債が溜まって改修しようと思った際に、そもそもどんな目的でこれは作られたのか、が大事な判断材料となり得ます。思わぬ影響範囲に気づくことにもあります。
ベンチャーではいかにユーザーに早く価値を提供できるか、というのが最も大事です。そもそもこれが良い価値だっけ? を考えるきっかけとなり、メンバーにわかりやすい形で共有することで一丸となって同じ方向を向けてスピードが出る、時間が経った後に選択を迫られた際に振り返れることでロスの多い選択をしない、これらの意味で大事だなあと思いました。</description>
    </item>
    
    <item>
      <title>F1 2018のTelemetryをGoでパースしてInfluxDBに流してGrafanaで描画してみる</title>
      <link>https://nito95.github.io/posts/2018-12-11/</link>
      <pubDate>Tue, 11 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://nito95.github.io/posts/2018-12-11/</guid>
      <description>今いるベンチャーでは車に搭載するIoTデバイスから加速度、GPS、ジャイロデータをリアルタイムで集めて色々と分析計算しソリューションを提供しています。そのデータの格納先に時系列データベースを使わないかと議論されているので、時系列データベースを自分で試してみたいなと思ってました。
会社の人の影響でF1 2018というゲームに最近ハマっているのですが、このゲームにはゲーム内のデータをUDPプロトコルで送信できるテレメトリー機能が搭載されています。 ゲーム内のF1カーの速度、スロットル/ブレーキ値、ステアリング値、燃料残量、GPSデータ、ラップタイムなどあらゆる情報がリアルタイムで取れます。
これはj時系列データベースを試してみるいいきっかけだなと思い、これらのデータを取得して時系列データベースであるInfluxDBに流し、Grafanaで描画するというのを作ってみました。F1 2018からUDPでMacへ → GoでパースしてInfluxDBへ → Grafanaで表示 という流れ。
コードはこちら nito95/F1-Telemetry
何のデータが取れるのかはこちらを参考にしました F1 2017 D-Box and UDP Output Specification
できたもの↓
  右のMacにリアルタイムで描画されているのはGrafanaの画面です。ここで表示しているのは速度とスロットル/ブレーキ値です。こんな感じ↓
  これだけの情報では示唆は少ないですが、ブレーキをフルであまり踏めてないとかブレーキ後のスロットルの踏みがたまに甘いとか多少考察できます。運転しながらリアルタイムで見るのはキツイですが後から振り返るには使えます。
F1エンジニアの人たちはこういった情報を見てレーサーに指示したりマシンのパーツ交換をしたりするんですね。F1エンジニアに興味が出ております。F1を見に行ったことはないですが、観客もテレメトリー情報をリアルタイムで見れたりするんですかね? 見れたらすごい観戦が楽しそうな気がします :)
あとInfluxDB + Grafanaは使いやすく、特に詰まることなくできました。Grafana見た目かっこいいです。これからも使ってみたいと思います。</description>
    </item>
    
  </channel>
</rss>
