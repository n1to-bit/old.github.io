<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>AWS Auroraは何がすごいのか | nito95 blog</title><meta name=keywords content="AWS"><meta name=description content="最近お手伝いしているベンチャーでRDS PostgreSQLをAuroraに移行しました。本番環境がスタンバイレプリカがない状態でRDSインスタンス1台で動いており、可用性を高めた方がいいのでどうせならAuroraに移行しちゃいましょう、と提案しエイヤと移行しました。
その話を友人にしたら、「Auroraって何がすごいん?」と質問されました。前職でも当然のようにAuroraを使っており、Auroraは可用性が高くwriteヘビーなサービスにも強くて少しコストが高い、というザックリしたイメージしかなかったので、フワっとした返答しかできませんでした。
ちゃんと調べてみようと思い、論文 Amazon Aurora: Design Considerations for High Throughput Cloud-Native Relational Databases を読んで後日改めて友人に説明したので、そのノリでここにもまとめてみます。
耐障害性とQuorum ストレージは1つで稼働させていると、何かの障害でそのストレージが止まってしまうとサービス全体が止まってしまうことになります。それを回避するために同じシステム環境を2つ用意し、1つは稼働し1つは待機させ、障害が発生した際には待機させていたストレージに稼働を切り替えるだけでサービスが再開するようにして可用性を高めます。いわゆるレプリケーションです。
この2つは常に同じデータを保持していなければなりません。なのでデータを書き込む際には両方に対して書き込みをするわけですが、片方のストレージで障害が起きたりネットワークの不通で書き込みに失敗した場合には2つのデータの整合性を保ちたいのでリトライが行われます。
しかしパフォーマンス面からするとリトライ処理は悪なので、1つくらいストレージが不調でレスポンスが返ってこなくても気にせず進めたい。データの整合性は保ちつつパフォーマンスも下げたくないとなると、解決策としては3つストレージを配置することとなります。 書き込みで3つ並列にリクエストが行い、1つ不調だったとしても2つからレスポンスが返ってこれば不整合なく書き込みが成功したとし、読み込みは3つにリクエストを行い2つから同じレスポンスが返ればそれは最新のデータだとします。多数決が行われるわけです。1つ遅いストレージ(straggler)がいても残り2つが正常なら素早い合意が取れるのでパフォーマンスの面でもメリットがあります。
分散システムにおいて処理の整合性をとるために多数決でもって合意を行うことをQuorum（クオラム）モデルと言います。
しかしここで終わりません。3つに分散させて2/3 Quorumで合意する仕組みでは不十分だとしました。
Auroraは6台構成 AWSの各リージョンには必ず3つ以上のAZ(アベイラビリティーゾーン)があります。AZ同士は物理的に距離が離れており、使われている電源系統も違えばソフトウェアのデプロイ周期も違います。 AWSの障害はAZ単位で起こることが多いので、サーバーを2つ作る場合は1つはAZ-aに置いて、もう1つはAZ-bに置けばAZ-aで障害が起きてもサービスは止まらずに済みます。
先ほど書いた2/3 Quorumでデータベースを構築するならば、AZ-aとAZ-bとAZ-cにそれぞれストレージを1つずつ置きます。ここでもしAZ-aで大規模な火災が起きたらどうなるでしょうか。復旧は数日ではできないので、しばらくAZ-bとAZ-cだけで稼働することになります。ここでさらにAZ-bに置いたストレージ1つに一時的な障害が起きてしまうと、稼働できるのがAZ-cだけとなり、読み込みも書き込みも2/3を満たせず合意が取れなくなります。
このAZ+1障害(AZレベルの障害が起き、さらにストレージ1つに障害が起きる)に対応するためにAuroraでは6台構成とし、4/6の書き込みQuorumと3/6の読み込みQuorumとしました。 各AZに2台ずつ置くことでAZ障害が起きても4つのストレージで読み書きでき、さらに1つ不調となっても3つのストレージで読み込みだけはできるようになります。
なぜ書き込みが4/6で読み込みが3/6なのか　 全体の投票数(ストレージ数)を \(V\)とし、読み込みQuorumの投票数を\(V_{r}\) 書き込みQuorumの投票数を\(V_{w}\)とします。
書き込みでは最新のデータを競合を回避して書き込めることが求められるので、過半数である\(V_{w} > V/2\)を満たす必要があります。もし半分だったらどっちが正解かわからなくなります。一方、読み込みは最新の書き込まれたデータを読むことが求められるので \(V_{r} + V_{w} > V\)を満たす必要があります。書き込みでは過半数を超えたら成功するので、このルールにすることで読み込みQuorumの中には少なくとも1つは最新のデータが必ず含まれることになります。
元々 hoge というデータが6つのストレージそれぞれに入っている状態で、 fuga に書き換えるとします。6つのうち4つは書き込みが成功した一方で2つは書き込みが失敗しました。結果的には過半数を超えるので書き込み成功です。すると次に読み込みを行うと4つからは fuga 2つからは hoge が返ってきます。最新データの取得を保証する意味においては3つから fuga が返ってこれば問題ないわけです。3つから hoge 3つから fuga が返ってくることは書き込みが成功している時点であり得ません。
  分散させることで生じるNetwork I/Oの問題と解決策 ストレージを複数に分散させる場合、問題になるのはNetworkです。従来のレプリケーションされたMySQLでは、書き込みのためにredo log、binlog、doublewrite用のデータページ、メタデータ(FRM)を送る必要があるため、6台構成ではとんでもない量のトラフィックとなってしまいます。 そのためAuroraでは、書き込みの際にネットワークを超えて送られるデータはredo logのみとし、ストレージ層にlog applicatorを置き各ストレージが自立的にredo logを再生することで最新状態までデータリカバリーできるようにしています。
Write-Ahead Log(WAL)とRedo Logとは WALは日本語にするとログ先行書き込みで、データベースに対する処理を行う前に全てログに記録する手法をいいます。例えばUPDATEをかけた場合、データをディスクから読み出してメモリ上で値を変えてディスクにフラッシュする必要があります。これが大量のレコードだった場合はデータをある程度の大きさのチャンクに分けてその塊ごとにUPDATEをかけていきます。 もしその途中で電源が落ちた場合、一部だけ書き換わっていて残りは古いままとなる不整合が発生します。それを防ぐために、書き込みをする前にどのページをどう書き換えるか操作情報を全てログに書きます。電源が落ちて再び電源を入れる際にクラッシュリカバリが走り、ログをチェックし、本来行うべき内容と実際に行われた内容を比較して処理を開始時までundoするか、redoして再開させるか判断することができます。ファイルシステムでいうジャーナリングと似てます。"><meta name=author content><link rel=canonical href=https://nito95.github.io/posts/2021-07-11/><meta name=google-site-verification content="XYZabc"><link rel=stylesheet href=https://nito95.github.io/css/custom.css><link crossorigin=anonymous href=/assets/css/stylesheet.min.35cd0f65a15cafa92372b8313deef5960aae04b90ad722f2bbf509eb0468137e.css integrity="sha256-Nc0PZaFcr6kjcrgxPe71lgquBLkK1yLyu/UJ6wRoE34=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://nito95.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://nito95.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://nito95.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://nito95.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://nito95.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.85.0"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css integrity=sha384-Um5gpz1odJg5Z4HAmzPtgZKdTBHZdw8S29IecapCSB31ligYPhHQZMIlWLYQGVoc crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.js integrity=sha384-YNHdsYkH6gMx9y3mRkmcJ2mFUjTd0qNQQvY9VYZgQd7DcN7env35GzlmFaZ23JGp crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/contrib/auto-render.min.js integrity=sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl crossorigin=anonymous onload=renderMathInElement(document.body)></script><meta property="og:title" content="AWS Auroraは何がすごいのか"><meta property="og:description" content="最近お手伝いしているベンチャーでRDS PostgreSQLをAuroraに移行しました。本番環境がスタンバイレプリカがない状態でRDSインスタンス1台で動いており、可用性を高めた方がいいのでどうせならAuroraに移行しちゃいましょう、と提案しエイヤと移行しました。
その話を友人にしたら、「Auroraって何がすごいん?」と質問されました。前職でも当然のようにAuroraを使っており、Auroraは可用性が高くwriteヘビーなサービスにも強くて少しコストが高い、というザックリしたイメージしかなかったので、フワっとした返答しかできませんでした。
ちゃんと調べてみようと思い、論文 Amazon Aurora: Design Considerations for High Throughput Cloud-Native Relational Databases を読んで後日改めて友人に説明したので、そのノリでここにもまとめてみます。
耐障害性とQuorum ストレージは1つで稼働させていると、何かの障害でそのストレージが止まってしまうとサービス全体が止まってしまうことになります。それを回避するために同じシステム環境を2つ用意し、1つは稼働し1つは待機させ、障害が発生した際には待機させていたストレージに稼働を切り替えるだけでサービスが再開するようにして可用性を高めます。いわゆるレプリケーションです。
この2つは常に同じデータを保持していなければなりません。なのでデータを書き込む際には両方に対して書き込みをするわけですが、片方のストレージで障害が起きたりネットワークの不通で書き込みに失敗した場合には2つのデータの整合性を保ちたいのでリトライが行われます。
しかしパフォーマンス面からするとリトライ処理は悪なので、1つくらいストレージが不調でレスポンスが返ってこなくても気にせず進めたい。データの整合性は保ちつつパフォーマンスも下げたくないとなると、解決策としては3つストレージを配置することとなります。 書き込みで3つ並列にリクエストが行い、1つ不調だったとしても2つからレスポンスが返ってこれば不整合なく書き込みが成功したとし、読み込みは3つにリクエストを行い2つから同じレスポンスが返ればそれは最新のデータだとします。多数決が行われるわけです。1つ遅いストレージ(straggler)がいても残り2つが正常なら素早い合意が取れるのでパフォーマンスの面でもメリットがあります。
分散システムにおいて処理の整合性をとるために多数決でもって合意を行うことをQuorum（クオラム）モデルと言います。
しかしここで終わりません。3つに分散させて2/3 Quorumで合意する仕組みでは不十分だとしました。
Auroraは6台構成 AWSの各リージョンには必ず3つ以上のAZ(アベイラビリティーゾーン)があります。AZ同士は物理的に距離が離れており、使われている電源系統も違えばソフトウェアのデプロイ周期も違います。 AWSの障害はAZ単位で起こることが多いので、サーバーを2つ作る場合は1つはAZ-aに置いて、もう1つはAZ-bに置けばAZ-aで障害が起きてもサービスは止まらずに済みます。
先ほど書いた2/3 Quorumでデータベースを構築するならば、AZ-aとAZ-bとAZ-cにそれぞれストレージを1つずつ置きます。ここでもしAZ-aで大規模な火災が起きたらどうなるでしょうか。復旧は数日ではできないので、しばらくAZ-bとAZ-cだけで稼働することになります。ここでさらにAZ-bに置いたストレージ1つに一時的な障害が起きてしまうと、稼働できるのがAZ-cだけとなり、読み込みも書き込みも2/3を満たせず合意が取れなくなります。
このAZ+1障害(AZレベルの障害が起き、さらにストレージ1つに障害が起きる)に対応するためにAuroraでは6台構成とし、4/6の書き込みQuorumと3/6の読み込みQuorumとしました。 各AZに2台ずつ置くことでAZ障害が起きても4つのストレージで読み書きでき、さらに1つ不調となっても3つのストレージで読み込みだけはできるようになります。
なぜ書き込みが4/6で読み込みが3/6なのか　 全体の投票数(ストレージ数)を \(V\)とし、読み込みQuorumの投票数を\(V_{r}\) 書き込みQuorumの投票数を\(V_{w}\)とします。
書き込みでは最新のデータを競合を回避して書き込めることが求められるので、過半数である\(V_{w} > V/2\)を満たす必要があります。もし半分だったらどっちが正解かわからなくなります。一方、読み込みは最新の書き込まれたデータを読むことが求められるので \(V_{r} + V_{w} > V\)を満たす必要があります。書き込みでは過半数を超えたら成功するので、このルールにすることで読み込みQuorumの中には少なくとも1つは最新のデータが必ず含まれることになります。
元々 hoge というデータが6つのストレージそれぞれに入っている状態で、 fuga に書き換えるとします。6つのうち4つは書き込みが成功した一方で2つは書き込みが失敗しました。結果的には過半数を超えるので書き込み成功です。すると次に読み込みを行うと4つからは fuga 2つからは hoge が返ってきます。最新データの取得を保証する意味においては3つから fuga が返ってこれば問題ないわけです。3つから hoge 3つから fuga が返ってくることは書き込みが成功している時点であり得ません。
  分散させることで生じるNetwork I/Oの問題と解決策 ストレージを複数に分散させる場合、問題になるのはNetworkです。従来のレプリケーションされたMySQLでは、書き込みのためにredo log、binlog、doublewrite用のデータページ、メタデータ(FRM)を送る必要があるため、6台構成ではとんでもない量のトラフィックとなってしまいます。 そのためAuroraでは、書き込みの際にネットワークを超えて送られるデータはredo logのみとし、ストレージ層にlog applicatorを置き各ストレージが自立的にredo logを再生することで最新状態までデータリカバリーできるようにしています。
Write-Ahead Log(WAL)とRedo Logとは WALは日本語にするとログ先行書き込みで、データベースに対する処理を行う前に全てログに記録する手法をいいます。例えばUPDATEをかけた場合、データをディスクから読み出してメモリ上で値を変えてディスクにフラッシュする必要があります。これが大量のレコードだった場合はデータをある程度の大きさのチャンクに分けてその塊ごとにUPDATEをかけていきます。 もしその途中で電源が落ちた場合、一部だけ書き換わっていて残りは古いままとなる不整合が発生します。それを防ぐために、書き込みをする前にどのページをどう書き換えるか操作情報を全てログに書きます。電源が落ちて再び電源を入れる際にクラッシュリカバリが走り、ログをチェックし、本来行うべき内容と実際に行われた内容を比較して処理を開始時までundoするか、redoして再開させるか判断することができます。ファイルシステムでいうジャーナリングと似てます。"><meta property="og:type" content="article"><meta property="og:url" content="https://nito95.github.io/posts/2021-07-11/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-07-11T00:00:00+00:00"><meta property="article:modified_time" content="2021-07-11T00:00:00+00:00"><meta property="og:site_name" content="nito95"><meta name=twitter:card content="summary"><meta name=twitter:title content="AWS Auroraは何がすごいのか"><meta name=twitter:description content="最近お手伝いしているベンチャーでRDS PostgreSQLをAuroraに移行しました。本番環境がスタンバイレプリカがない状態でRDSインスタンス1台で動いており、可用性を高めた方がいいのでどうせならAuroraに移行しちゃいましょう、と提案しエイヤと移行しました。
その話を友人にしたら、「Auroraって何がすごいん?」と質問されました。前職でも当然のようにAuroraを使っており、Auroraは可用性が高くwriteヘビーなサービスにも強くて少しコストが高い、というザックリしたイメージしかなかったので、フワっとした返答しかできませんでした。
ちゃんと調べてみようと思い、論文 Amazon Aurora: Design Considerations for High Throughput Cloud-Native Relational Databases を読んで後日改めて友人に説明したので、そのノリでここにもまとめてみます。
耐障害性とQuorum ストレージは1つで稼働させていると、何かの障害でそのストレージが止まってしまうとサービス全体が止まってしまうことになります。それを回避するために同じシステム環境を2つ用意し、1つは稼働し1つは待機させ、障害が発生した際には待機させていたストレージに稼働を切り替えるだけでサービスが再開するようにして可用性を高めます。いわゆるレプリケーションです。
この2つは常に同じデータを保持していなければなりません。なのでデータを書き込む際には両方に対して書き込みをするわけですが、片方のストレージで障害が起きたりネットワークの不通で書き込みに失敗した場合には2つのデータの整合性を保ちたいのでリトライが行われます。
しかしパフォーマンス面からするとリトライ処理は悪なので、1つくらいストレージが不調でレスポンスが返ってこなくても気にせず進めたい。データの整合性は保ちつつパフォーマンスも下げたくないとなると、解決策としては3つストレージを配置することとなります。 書き込みで3つ並列にリクエストが行い、1つ不調だったとしても2つからレスポンスが返ってこれば不整合なく書き込みが成功したとし、読み込みは3つにリクエストを行い2つから同じレスポンスが返ればそれは最新のデータだとします。多数決が行われるわけです。1つ遅いストレージ(straggler)がいても残り2つが正常なら素早い合意が取れるのでパフォーマンスの面でもメリットがあります。
分散システムにおいて処理の整合性をとるために多数決でもって合意を行うことをQuorum（クオラム）モデルと言います。
しかしここで終わりません。3つに分散させて2/3 Quorumで合意する仕組みでは不十分だとしました。
Auroraは6台構成 AWSの各リージョンには必ず3つ以上のAZ(アベイラビリティーゾーン)があります。AZ同士は物理的に距離が離れており、使われている電源系統も違えばソフトウェアのデプロイ周期も違います。 AWSの障害はAZ単位で起こることが多いので、サーバーを2つ作る場合は1つはAZ-aに置いて、もう1つはAZ-bに置けばAZ-aで障害が起きてもサービスは止まらずに済みます。
先ほど書いた2/3 Quorumでデータベースを構築するならば、AZ-aとAZ-bとAZ-cにそれぞれストレージを1つずつ置きます。ここでもしAZ-aで大規模な火災が起きたらどうなるでしょうか。復旧は数日ではできないので、しばらくAZ-bとAZ-cだけで稼働することになります。ここでさらにAZ-bに置いたストレージ1つに一時的な障害が起きてしまうと、稼働できるのがAZ-cだけとなり、読み込みも書き込みも2/3を満たせず合意が取れなくなります。
このAZ+1障害(AZレベルの障害が起き、さらにストレージ1つに障害が起きる)に対応するためにAuroraでは6台構成とし、4/6の書き込みQuorumと3/6の読み込みQuorumとしました。 各AZに2台ずつ置くことでAZ障害が起きても4つのストレージで読み書きでき、さらに1つ不調となっても3つのストレージで読み込みだけはできるようになります。
なぜ書き込みが4/6で読み込みが3/6なのか　 全体の投票数(ストレージ数)を \(V\)とし、読み込みQuorumの投票数を\(V_{r}\) 書き込みQuorumの投票数を\(V_{w}\)とします。
書き込みでは最新のデータを競合を回避して書き込めることが求められるので、過半数である\(V_{w} > V/2\)を満たす必要があります。もし半分だったらどっちが正解かわからなくなります。一方、読み込みは最新の書き込まれたデータを読むことが求められるので \(V_{r} + V_{w} > V\)を満たす必要があります。書き込みでは過半数を超えたら成功するので、このルールにすることで読み込みQuorumの中には少なくとも1つは最新のデータが必ず含まれることになります。
元々 hoge というデータが6つのストレージそれぞれに入っている状態で、 fuga に書き換えるとします。6つのうち4つは書き込みが成功した一方で2つは書き込みが失敗しました。結果的には過半数を超えるので書き込み成功です。すると次に読み込みを行うと4つからは fuga 2つからは hoge が返ってきます。最新データの取得を保証する意味においては3つから fuga が返ってこれば問題ないわけです。3つから hoge 3つから fuga が返ってくることは書き込みが成功している時点であり得ません。
  分散させることで生じるNetwork I/Oの問題と解決策 ストレージを複数に分散させる場合、問題になるのはNetworkです。従来のレプリケーションされたMySQLでは、書き込みのためにredo log、binlog、doublewrite用のデータページ、メタデータ(FRM)を送る必要があるため、6台構成ではとんでもない量のトラフィックとなってしまいます。 そのためAuroraでは、書き込みの際にネットワークを超えて送られるデータはredo logのみとし、ストレージ層にlog applicatorを置き各ストレージが自立的にredo logを再生することで最新状態までデータリカバリーできるようにしています。
Write-Ahead Log(WAL)とRedo Logとは WALは日本語にするとログ先行書き込みで、データベースに対する処理を行う前に全てログに記録する手法をいいます。例えばUPDATEをかけた場合、データをディスクから読み出してメモリ上で値を変えてディスクにフラッシュする必要があります。これが大量のレコードだった場合はデータをある程度の大きさのチャンクに分けてその塊ごとにUPDATEをかけていきます。 もしその途中で電源が落ちた場合、一部だけ書き換わっていて残りは古いままとなる不整合が発生します。それを防ぐために、書き込みをする前にどのページをどう書き換えるか操作情報を全てログに書きます。電源が落ちて再び電源を入れる際にクラッシュリカバリが走り、ログをチェックし、本来行うべき内容と実際に行われた内容を比較して処理を開始時までundoするか、redoして再開させるか判断することができます。ファイルシステムでいうジャーナリングと似てます。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://nito95.github.io/posts/"},{"@type":"ListItem","position":2,"name":"AWS Auroraは何がすごいのか","item":"https://nito95.github.io/posts/2021-07-11/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"AWS Auroraは何がすごいのか","name":"AWS Auroraは何がすごいのか","description":"最近お手伝いしているベンチャーでRDS PostgreSQLをAuroraに移行しました。本番環境がスタンバイレプリカがない状態でRDSインスタンス1台で動いており、可用性を高めた方がいいのでどうせならAuroraに移行しちゃいましょう、と提案しエイヤと移行しました。\nその話を友人にしたら、「Auroraって何がすごいん?」と質問されました。前職でも当然のようにAuroraを使っており、Auroraは可用性が高くwriteヘビーなサービスにも強くて少しコストが高い、というザックリしたイメージしかなかったので、フワっとした返答しかできませんでした。\nちゃんと調べてみようと思い、論文 Amazon Aurora: Design Considerations for High Throughput Cloud-Native Relational Databases を読んで後日改めて友人に説明したので、そのノリでここにもまとめてみます。\n耐障害性とQuorum ストレージは1つで稼働させていると、何かの障害でそのストレージが止まってしまうとサービス全体が止まってしまうことになります。それを回避するために同じシステム環境を2つ用意し、1つは稼働し1つは待機させ、障害が発生した際には待機させていたストレージに稼働を切り替えるだけでサービスが再開するようにして可用性を高めます。いわゆるレプリケーションです。\nこの2つは常に同じデータを保持していなければなりません。なのでデータを書き込む際には両方に対して書き込みをするわけですが、片方のストレージで障害が起きたりネットワークの不通で書き込みに失敗した場合には2つのデータの整合性を保ちたいのでリトライが行われます。\nしかしパフォーマンス面からするとリトライ処理は悪なので、1つくらいストレージが不調でレスポンスが返ってこなくても気にせず進めたい。データの整合性は保ちつつパフォーマンスも下げたくないとなると、解決策としては3つストレージを配置することとなります。 書き込みで3つ並列にリクエストが行い、1つ不調だったとしても2つからレスポンスが返ってこれば不整合なく書き込みが成功したとし、読み込みは3つにリクエストを行い2つから同じレスポンスが返ればそれは最新のデータだとします。多数決が行われるわけです。1つ遅いストレージ(straggler)がいても残り2つが正常なら素早い合意が取れるのでパフォーマンスの面でもメリットがあります。\n分散システムにおいて処理の整合性をとるために多数決でもって合意を行うことをQuorum（クオラム）モデルと言います。\nしかしここで終わりません。3つに分散させて2/3 Quorumで合意する仕組みでは不十分だとしました。\nAuroraは6台構成 AWSの各リージョンには必ず3つ以上のAZ(アベイラビリティーゾーン)があります。AZ同士は物理的に距離が離れており、使われている電源系統も違えばソフトウェアのデプロイ周期も違います。 AWSの障害はAZ単位で起こることが多いので、サーバーを2つ作る場合は1つはAZ-aに置いて、もう1つはAZ-bに置けばAZ-aで障害が起きてもサービスは止まらずに済みます。\n先ほど書いた2/3 Quorumでデータベースを構築するならば、AZ-aとAZ-bとAZ-cにそれぞれストレージを1つずつ置きます。ここでもしAZ-aで大規模な火災が起きたらどうなるでしょうか。復旧は数日ではできないので、しばらくAZ-bとAZ-cだけで稼働することになります。ここでさらにAZ-bに置いたストレージ1つに一時的な障害が起きてしまうと、稼働できるのがAZ-cだけとなり、読み込みも書き込みも2/3を満たせず合意が取れなくなります。\nこのAZ+1障害(AZレベルの障害が起き、さらにストレージ1つに障害が起きる)に対応するためにAuroraでは6台構成とし、4/6の書き込みQuorumと3/6の読み込みQuorumとしました。 各AZに2台ずつ置くことでAZ障害が起きても4つのストレージで読み書きでき、さらに1つ不調となっても3つのストレージで読み込みだけはできるようになります。\nなぜ書き込みが4/6で読み込みが3/6なのか　 全体の投票数(ストレージ数)を \\(V\\)とし、読み込みQuorumの投票数を\\(V_{r}\\) 書き込みQuorumの投票数を\\(V_{w}\\)とします。\n書き込みでは最新のデータを競合を回避して書き込めることが求められるので、過半数である\\(V_{w} \u0026gt; V/2\\)を満たす必要があります。もし半分だったらどっちが正解かわからなくなります。一方、読み込みは最新の書き込まれたデータを読むことが求められるので \\(V_{r} + V_{w} \u0026gt; V\\)を満たす必要があります。書き込みでは過半数を超えたら成功するので、このルールにすることで読み込みQuorumの中には少なくとも1つは最新のデータが必ず含まれることになります。\n元々 hoge というデータが6つのストレージそれぞれに入っている状態で、 fuga に書き換えるとします。6つのうち4つは書き込みが成功した一方で2つは書き込みが失敗しました。結果的には過半数を超えるので書き込み成功です。すると次に読み込みを行うと4つからは fuga 2つからは hoge が返ってきます。最新データの取得を保証する意味においては3つから fuga が返ってこれば問題ないわけです。3つから hoge 3つから fuga が返ってくることは書き込みが成功している時点であり得ません。\n  分散させることで生じるNetwork I/Oの問題と解決策 ストレージを複数に分散させる場合、問題になるのはNetworkです。従来のレプリケーションされたMySQLでは、書き込みのためにredo log、binlog、doublewrite用のデータページ、メタデータ(FRM)を送る必要があるため、6台構成ではとんでもない量のトラフィックとなってしまいます。 そのためAuroraでは、書き込みの際にネットワークを超えて送られるデータはredo logのみとし、ストレージ層にlog applicatorを置き各ストレージが自立的にredo logを再生することで最新状態までデータリカバリーできるようにしています。\nWrite-Ahead Log(WAL)とRedo Logとは WALは日本語にするとログ先行書き込みで、データベースに対する処理を行う前に全てログに記録する手法をいいます。例えばUPDATEをかけた場合、データをディスクから読み出してメモリ上で値を変えてディスクにフラッシュする必要があります。これが大量のレコードだった場合はデータをある程度の大きさのチャンクに分けてその塊ごとにUPDATEをかけていきます。 もしその途中で電源が落ちた場合、一部だけ書き換わっていて残りは古いままとなる不整合が発生します。それを防ぐために、書き込みをする前にどのページをどう書き換えるか操作情報を全てログに書きます。電源が落ちて再び電源を入れる際にクラッシュリカバリが走り、ログをチェックし、本来行うべき内容と実際に行われた内容を比較して処理を開始時までundoするか、redoして再開させるか判断することができます。ファイルシステムでいうジャーナリングと似てます。","keywords":["AWS"],"articleBody":"最近お手伝いしているベンチャーでRDS PostgreSQLをAuroraに移行しました。本番環境がスタンバイレプリカがない状態でRDSインスタンス1台で動いており、可用性を高めた方がいいのでどうせならAuroraに移行しちゃいましょう、と提案しエイヤと移行しました。\nその話を友人にしたら、「Auroraって何がすごいん?」と質問されました。前職でも当然のようにAuroraを使っており、Auroraは可用性が高くwriteヘビーなサービスにも強くて少しコストが高い、というザックリしたイメージしかなかったので、フワっとした返答しかできませんでした。\nちゃんと調べてみようと思い、論文 Amazon Aurora: Design Considerations for High Throughput Cloud-Native Relational Databases を読んで後日改めて友人に説明したので、そのノリでここにもまとめてみます。\n耐障害性とQuorum ストレージは1つで稼働させていると、何かの障害でそのストレージが止まってしまうとサービス全体が止まってしまうことになります。それを回避するために同じシステム環境を2つ用意し、1つは稼働し1つは待機させ、障害が発生した際には待機させていたストレージに稼働を切り替えるだけでサービスが再開するようにして可用性を高めます。いわゆるレプリケーションです。\nこの2つは常に同じデータを保持していなければなりません。なのでデータを書き込む際には両方に対して書き込みをするわけですが、片方のストレージで障害が起きたりネットワークの不通で書き込みに失敗した場合には2つのデータの整合性を保ちたいのでリトライが行われます。\nしかしパフォーマンス面からするとリトライ処理は悪なので、1つくらいストレージが不調でレスポンスが返ってこなくても気にせず進めたい。データの整合性は保ちつつパフォーマンスも下げたくないとなると、解決策としては3つストレージを配置することとなります。 書き込みで3つ並列にリクエストが行い、1つ不調だったとしても2つからレスポンスが返ってこれば不整合なく書き込みが成功したとし、読み込みは3つにリクエストを行い2つから同じレスポンスが返ればそれは最新のデータだとします。多数決が行われるわけです。1つ遅いストレージ(straggler)がいても残り2つが正常なら素早い合意が取れるのでパフォーマンスの面でもメリットがあります。\n分散システムにおいて処理の整合性をとるために多数決でもって合意を行うことをQuorum（クオラム）モデルと言います。\nしかしここで終わりません。3つに分散させて2/3 Quorumで合意する仕組みでは不十分だとしました。\nAuroraは6台構成 AWSの各リージョンには必ず3つ以上のAZ(アベイラビリティーゾーン)があります。AZ同士は物理的に距離が離れており、使われている電源系統も違えばソフトウェアのデプロイ周期も違います。 AWSの障害はAZ単位で起こることが多いので、サーバーを2つ作る場合は1つはAZ-aに置いて、もう1つはAZ-bに置けばAZ-aで障害が起きてもサービスは止まらずに済みます。\n先ほど書いた2/3 Quorumでデータベースを構築するならば、AZ-aとAZ-bとAZ-cにそれぞれストレージを1つずつ置きます。ここでもしAZ-aで大規模な火災が起きたらどうなるでしょうか。復旧は数日ではできないので、しばらくAZ-bとAZ-cだけで稼働することになります。ここでさらにAZ-bに置いたストレージ1つに一時的な障害が起きてしまうと、稼働できるのがAZ-cだけとなり、読み込みも書き込みも2/3を満たせず合意が取れなくなります。\nこのAZ+1障害(AZレベルの障害が起き、さらにストレージ1つに障害が起きる)に対応するためにAuroraでは6台構成とし、4/6の書き込みQuorumと3/6の読み込みQuorumとしました。 各AZに2台ずつ置くことでAZ障害が起きても4つのストレージで読み書きでき、さらに1つ不調となっても3つのストレージで読み込みだけはできるようになります。\nなぜ書き込みが4/6で読み込みが3/6なのか　 全体の投票数(ストレージ数)を \\(V\\)とし、読み込みQuorumの投票数を\\(V_{r}\\) 書き込みQuorumの投票数を\\(V_{w}\\)とします。\n書き込みでは最新のデータを競合を回避して書き込めることが求められるので、過半数である\\(V_{w}  V/2\\)を満たす必要があります。もし半分だったらどっちが正解かわからなくなります。一方、読み込みは最新の書き込まれたデータを読むことが求められるので \\(V_{r} + V_{w}  V\\)を満たす必要があります。書き込みでは過半数を超えたら成功するので、このルールにすることで読み込みQuorumの中には少なくとも1つは最新のデータが必ず含まれることになります。\n元々 hoge というデータが6つのストレージそれぞれに入っている状態で、 fuga に書き換えるとします。6つのうち4つは書き込みが成功した一方で2つは書き込みが失敗しました。結果的には過半数を超えるので書き込み成功です。すると次に読み込みを行うと4つからは fuga 2つからは hoge が返ってきます。最新データの取得を保証する意味においては3つから fuga が返ってこれば問題ないわけです。3つから hoge 3つから fuga が返ってくることは書き込みが成功している時点であり得ません。\n  分散させることで生じるNetwork I/Oの問題と解決策 ストレージを複数に分散させる場合、問題になるのはNetworkです。従来のレプリケーションされたMySQLでは、書き込みのためにredo log、binlog、doublewrite用のデータページ、メタデータ(FRM)を送る必要があるため、6台構成ではとんでもない量のトラフィックとなってしまいます。 そのためAuroraでは、書き込みの際にネットワークを超えて送られるデータはredo logのみとし、ストレージ層にlog applicatorを置き各ストレージが自立的にredo logを再生することで最新状態までデータリカバリーできるようにしています。\nWrite-Ahead Log(WAL)とRedo Logとは WALは日本語にするとログ先行書き込みで、データベースに対する処理を行う前に全てログに記録する手法をいいます。例えばUPDATEをかけた場合、データをディスクから読み出してメモリ上で値を変えてディスクにフラッシュする必要があります。これが大量のレコードだった場合はデータをある程度の大きさのチャンクに分けてその塊ごとにUPDATEをかけていきます。 もしその途中で電源が落ちた場合、一部だけ書き換わっていて残りは古いままとなる不整合が発生します。それを防ぐために、書き込みをする前にどのページをどう書き換えるか操作情報を全てログに書きます。電源が落ちて再び電源を入れる際にクラッシュリカバリが走り、ログをチェックし、本来行うべき内容と実際に行われた内容を比較して処理を開始時までundoするか、redoして再開させるか判断することができます。ファイルシステムでいうジャーナリングと似てます。\nredoに使うためのログがredo logです。ちなみにGitのコミットログも、どう変更されたかの差分情報だけなのでredo logです。 Auroraではストレージが新たに追加された場合や再起動された場合にログを取得し再生します。これによって従来あったクラッシュリカバリの諸々の処理を不要にできます。\nちなみにredo logを取りこぼしてしまってもPrimaryに聞きに行くことはせず、他のストレージとGossip通信を行って同期させます。\nまとめ Auroraの興味深い点は、6台構成にして3/6読み込みQuorumと4/6書き込みQuorumで合意形成をしている点、Primaryはredo logを投げ続けるだけで各ストレージたちが自立的にログを再生してデータを最新にできる点でした。 この論文を読んでMySQLとPostgreSQLの既存の仕組みを調べることにもなったので、知識が深まるいいきっかけとなりました。\n書籍 Nand2Tetris(コンピュータシステムの理論と実装) を読んで以来、腹落ちさせるには読むだけでなく簡単なものでいいので自作するのが良いと思っているので、簡単なデータベースを自作してみたいなという気持ちが沸いております。\n","wordCount":"83","inLanguage":"en","datePublished":"2021-07-11T00:00:00Z","dateModified":"2021-07-11T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://nito95.github.io/posts/2021-07-11/"},"publisher":{"@type":"Organization","name":"nito95 blog","logo":{"@type":"ImageObject","url":"https://nito95.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add('dark')</script><noscript><style type=text/css>#theme-toggle,.top-link{display:none}</style></noscript><header class=header><nav class=nav><div class=logo><a href=https://nito95.github.io/ accesskey=h title="nito95 blog (Alt + H)">nito95 blog</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://nito95.github.io/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>AWS Auroraは何がすごいのか</h1><div class=post-meta>July 11, 2021</div></header><div class=post-content><p>最近お手伝いしているベンチャーでRDS PostgreSQLをAuroraに移行しました。本番環境がスタンバイレプリカがない状態でRDSインスタンス1台で動いており、可用性を高めた方がいいのでどうせならAuroraに移行しちゃいましょう、と提案しエイヤと移行しました。</p><p>その話を友人にしたら、「Auroraって何がすごいん?」と質問されました。前職でも当然のようにAuroraを使っており、Auroraは可用性が高くwriteヘビーなサービスにも強くて少しコストが高い、というザックリしたイメージしかなかったので、フワっとした返答しかできませんでした。</p><p>ちゃんと調べてみようと思い、論文 <a href=https://www.allthingsdistributed.com/files/p1041-verbitski.pdf>Amazon Aurora: Design Considerations for High Throughput Cloud-Native Relational Databases</a> を読んで後日改めて友人に説明したので、そのノリでここにもまとめてみます。</p><h3 id=耐障害性とquorum>耐障害性とQuorum<a hidden class=anchor aria-hidden=true href=#耐障害性とquorum>#</a></h3><p>ストレージは1つで稼働させていると、何かの障害でそのストレージが止まってしまうとサービス全体が止まってしまうことになります。それを回避するために同じシステム環境を2つ用意し、1つは稼働し1つは待機させ、障害が発生した際には待機させていたストレージに稼働を切り替えるだけでサービスが再開するようにして可用性を高めます。いわゆるレプリケーションです。</p><p>この2つは常に同じデータを保持していなければなりません。なのでデータを書き込む際には両方に対して書き込みをするわけですが、片方のストレージで障害が起きたりネットワークの不通で書き込みに失敗した場合には2つのデータの整合性を保ちたいのでリトライが行われます。</p><p>しかしパフォーマンス面からするとリトライ処理は悪なので、1つくらいストレージが不調でレスポンスが返ってこなくても気にせず進めたい。データの整合性は保ちつつパフォーマンスも下げたくないとなると、解決策としては3つストレージを配置することとなります。
書き込みで3つ並列にリクエストが行い、1つ不調だったとしても2つからレスポンスが返ってこれば不整合なく書き込みが成功したとし、読み込みは3つにリクエストを行い2つから同じレスポンスが返ればそれは最新のデータだとします。多数決が行われるわけです。1つ遅いストレージ(straggler)がいても残り2つが正常なら素早い合意が取れるのでパフォーマンスの面でもメリットがあります。</p><p>分散システムにおいて処理の整合性をとるために多数決でもって合意を行うことを<a href=https://en.wikipedia.org/wiki/Quorum_(distributed_computing)>Quorum（クオラム）</a>モデルと言います。</p><p>しかしここで終わりません。3つに分散させて2/3 Quorumで合意する仕組みでは不十分だとしました。</p><h3 id=auroraは6台構成>Auroraは6台構成<a hidden class=anchor aria-hidden=true href=#auroraは6台構成>#</a></h3><p>AWSの各リージョンには必ず3つ以上のAZ(アベイラビリティーゾーン)があります。AZ同士は物理的に距離が離れており、使われている電源系統も違えばソフトウェアのデプロイ周期も違います。
AWSの障害はAZ単位で起こることが多いので、サーバーを2つ作る場合は1つはAZ-aに置いて、もう1つはAZ-bに置けばAZ-aで障害が起きてもサービスは止まらずに済みます。</p><p>先ほど書いた2/3 Quorumでデータベースを構築するならば、AZ-aとAZ-bとAZ-cにそれぞれストレージを1つずつ置きます。ここでもしAZ-aで大規模な火災が起きたらどうなるでしょうか。復旧は数日ではできないので、しばらくAZ-bとAZ-cだけで稼働することになります。ここでさらにAZ-bに置いたストレージ1つに一時的な障害が起きてしまうと、稼働できるのがAZ-cだけとなり、読み込みも書き込みも2/3を満たせず合意が取れなくなります。</p><p>このAZ+1障害(AZレベルの障害が起き、さらにストレージ1つに障害が起きる)に対応するためにAuroraでは6台構成とし、4/6の書き込みQuorumと3/6の読み込みQuorumとしました。
各AZに2台ずつ置くことでAZ障害が起きても4つのストレージで読み書きでき、さらに1つ不調となっても3つのストレージで読み込みだけはできるようになります。</p><h4 id=なぜ書き込みが46で読み込みが36なのか>なぜ書き込みが4/6で読み込みが3/6なのか　<a hidden class=anchor aria-hidden=true href=#なぜ書き込みが46で読み込みが36なのか>#</a></h4><p>全体の投票数(ストレージ数)を \(V\)とし、読み込みQuorumの投票数を\(V_{r}\) 書き込みQuorumの投票数を\(V_{w}\)とします。</p><p>書き込みでは最新のデータを競合を回避して書き込めることが求められるので、過半数である\(V_{w} > V/2\)を満たす必要があります。もし半分だったらどっちが正解かわからなくなります。一方、読み込みは最新の書き込まれたデータを読むことが求められるので \(V_{r} + V_{w} > V\)を満たす必要があります。書き込みでは過半数を超えたら成功するので、このルールにすることで読み込みQuorumの中には少なくとも1つは最新のデータが必ず含まれることになります。</p><p>元々 <code>hoge</code> というデータが6つのストレージそれぞれに入っている状態で、 <code>fuga</code> に書き換えるとします。6つのうち4つは書き込みが成功した一方で2つは書き込みが失敗しました。結果的には過半数を超えるので書き込み成功です。すると次に読み込みを行うと4つからは <code>fuga</code> 2つからは <code>hoge</code> が返ってきます。最新データの取得を保証する意味においては3つから <code>fuga</code> が返ってこれば問題ないわけです。3つから <code>hoge</code> 3つから <code>fuga</code> が返ってくることは書き込みが成功している時点であり得ません。</p><figure><img loading=lazy src=/images/2021-07-07-1.png width=360></figure><h3 id=分散させることで生じるnetwork-ioの問題と解決策>分散させることで生じるNetwork I/Oの問題と解決策<a hidden class=anchor aria-hidden=true href=#分散させることで生じるnetwork-ioの問題と解決策>#</a></h3><p>ストレージを複数に分散させる場合、問題になるのはNetworkです。従来のレプリケーションされたMySQLでは、書き込みのためにredo log、binlog、doublewrite用のデータページ、メタデータ(FRM)を送る必要があるため、6台構成ではとんでもない量のトラフィックとなってしまいます。
そのためAuroraでは、書き込みの際にネットワークを超えて送られるデータはredo logのみとし、ストレージ層にlog applicatorを置き各ストレージが自立的にredo logを再生することで最新状態までデータリカバリーできるようにしています。</p><h4 id=write-ahead-logwalとredo-logとは>Write-Ahead Log(WAL)とRedo Logとは<a hidden class=anchor aria-hidden=true href=#write-ahead-logwalとredo-logとは>#</a></h4><p>WALは日本語にするとログ先行書き込みで、データベースに対する処理を行う前に全てログに記録する手法をいいます。例えばUPDATEをかけた場合、データをディスクから読み出してメモリ上で値を変えてディスクにフラッシュする必要があります。これが大量のレコードだった場合はデータをある程度の大きさのチャンクに分けてその塊ごとにUPDATEをかけていきます。
もしその途中で電源が落ちた場合、一部だけ書き換わっていて残りは古いままとなる不整合が発生します。それを防ぐために、書き込みをする前にどのページをどう書き換えるか操作情報を全てログに書きます。電源が落ちて再び電源を入れる際にクラッシュリカバリが走り、ログをチェックし、本来行うべき内容と実際に行われた内容を比較して処理を開始時までundoするか、redoして再開させるか判断することができます。ファイルシステムでいう<a href=https://ja.wikipedia.org/wiki/%E3%82%B8%E3%83%A3%E3%83%BC%E3%83%8A%E3%83%AA%E3%83%B3%E3%82%B0%E3%83%95%E3%82%A1%E3%82%A4%E3%83%AB%E3%82%B7%E3%82%B9%E3%83%86%E3%83%A0>ジャーナリング</a>と似てます。</p><p>redoに使うためのログがredo logです。ちなみにGitのコミットログも、どう変更されたかの差分情報だけなのでredo logです。
Auroraではストレージが新たに追加された場合や再起動された場合にログを取得し再生します。これによって従来あったクラッシュリカバリの諸々の処理を不要にできます。</p><p>ちなみにredo logを取りこぼしてしまってもPrimaryに聞きに行くことはせず、他のストレージとGossip通信を行って同期させます。</p><h3 id=まとめ>まとめ<a hidden class=anchor aria-hidden=true href=#まとめ>#</a></h3><p>Auroraの興味深い点は、6台構成にして3/6読み込みQuorumと4/6書き込みQuorumで合意形成をしている点、Primaryはredo logを投げ続けるだけで各ストレージたちが自立的にログを再生してデータを最新にできる点でした。
この論文を読んでMySQLとPostgreSQLの既存の仕組みを調べることにもなったので、知識が深まるいいきっかけとなりました。</p><p>書籍 <a href=https://www.amazon.co.jp/dp/4873117127>Nand2Tetris(コンピュータシステムの理論と実装)</a> を読んで以来、腹落ちさせるには読むだけでなく簡単なものでいいので自作するのが良いと思っているので、簡単なデータベースを自作してみたいなという気持ちが沸いております。</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://nito95.github.io/tags/aws/>AWS</a></li></ul><nav class=paginav><a class=next href=https://nito95.github.io/posts/2021-07-02/><span class=title>Next Page »</span><br><span>C++の std::sort() で使われている Introsort とは</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2021 <a href=https://nito95.github.io/>nito95 blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)"><button class=top-link id=top-link type=button accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></button></a>
<script>let menu=document.getElementById('menu');menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)},document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script></body></html>